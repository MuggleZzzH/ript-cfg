defaults:
  - task: libero_spatial_rl
  - algo: pi0_cfg_rl
  - reward_function: success
  - paths
  - _self_

# set the path to your data and experiments directories
output_prefix: ${paths.output_prefix}
data_prefix: ${paths.data_prefix}

exp_name: pi0_cfg # 
variant_name: debug
seed: 10000
device: cuda
stage: null # 0 - pretrain autoencoder, 1 - train multitask, 2 - finetune multitask
make_unique_experiment_dir: true # if true, it will create unique experiment directories with name run_00X within that experiment directory, use when debugging
logging_folder: pi0_cfg

checkpoint_path: null

train_dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 2
  shuffle: true
  num_workers: 6
  persistent_workers: false
  pin_memory: true
  multiprocessing_context: fork
  drop_last: true
  # prefetch_factor: 2

training:
  gradient_accumulation_steps: 1
  n_epochs: 100
  save_interval: 10
  log_interval: 100
  use_amp: false
  use_tqdm: true
  do_profile: false
  save_all_checkpoints: false
  auto_continue: false # if true, it will automatically continue from the end of stage n training for stage n+1 training
  load_obs: true
  cut: 0
  n_steps: 20 # -1 if you want to train for n_epochs, otherwise train for n_steps
  rollout_steps: 2 # -1 if use rollout.interval, otherwise use rollout_steps
  grad_clip: 1.0

  # resume a training run
  resume: false
  resume_path: ""

dataset:
  seq_len: 600
  frame_stack: 1 # this denotes past timestep observations and actions
  obs_seq_len: 1 # this denotes future timestep image observations
  lowdim_obs_seq_len: null # this denotes future timestep lowdim observations
  load_obs_for_pretrain: true # since autoencoder training stage does not require obs
  load_next_obs: true # you know this
  get_pad_mask: true
  load_state: true

rollout:
  enabled: true
  interval: 10
  rollouts_per_env: 16
  num_parallel_envs: 1
  max_episode_length: 300

task:
  task_names_to_use: 
    - pick_up_the_black_bowl_next_to_the_plate_and_place_it_on_the_plate
  env_runner:
    num_parallel_envs: 1

logging:
  group: null
  mode: disabled # set logging.mode=disabled to disable wandb
  project: ${common.wandb_project}
  resume: true
  save_code: true
